{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build AI-powered semantic search applications txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. Traditional search systems use keywords to find data. Semantic search applications have an understanding of natural language and identify results that have the same meaning, not necessarily the same keywords. Backed by state-of-the-art machine learning models, data is transformed into vector representations for search (also known as embeddings). Innovation is happening at a rapid pace, models can understand concepts in documents, audio, images and more. Summary of txtai features: \ud83d\udd0e Large-scale similarity search with multiple index backends ( Faiss , Annoy , Hnswlib ) \ud83d\udcc4 Create embeddings for text snippets, documents, audio, images and video. Supports transformers and word vectors. \ud83d\udca1 Machine-learning pipelines to run extractive question-answering, zero-shot labeling, transcription, translation, summarization and text extraction \u21aa\ufe0f\ufe0f Workflows that join pipelines together to aggregate business logic. txtai processes can be microservices or full-fledged indexing workflows. \ud83d\udd17 API bindings for JavaScript , Java , Rust and Go \u2601\ufe0f Cloud-native architecture that scales out with container orchestration systems (e.g. Kubernetes) Applications range from similarity search to complex NLP-driven data extractions to generate structured databases. The following applications are powered by txtai. Application Description paperai AI-powered literature discovery and review engine for medical/scientific papers tldrstory AI-powered understanding of headlines and story text neuspo Fact-driven, real-time sports event and news site codequestion Ask coding questions directly from the terminal txtai is built with Python 3.6+, Hugging Face Transformers , Sentence Transformers and FastAPI","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"api/","text":"API txtai has a full-featured API that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note that this configuration file enables all functionality. It is suggested that separate processes are used for each instance of a txtai component. Components can be joined together with workflows. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Enbeddings index embeddings : path : sentence-transformers/nli-mpnet-base-v2 # Extractive QA extractor : path : distilbert-base-cased-distilled-squad # Zero-shot labeling labels : # Similarity similarity : # Text segmentation segmentation : sentences : true # Text summarization summary : # Text extraction textractor : paragraphs : true minlength : 100 join : true # Transcribe audio to text transcription : # Translate text between languages translation : # Workflow definitions workflow : sumfrench : tasks : - action : textractor task : storage ids : false - action : summary - action : translation args : [ \"fr\" ] sumspanish : tasks : - action : textractor task : url - action : summary - action : translation args : [ \"es\" ] Assuming this YAML content is stored in a file named index.yml, the following command starts the API process. CONFIG=index.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details. Docker A Dockerfile with commands to install txtai, all dependencies and default configuration is available in this repository. The Dockerfile can be copied from the docker directory on GitHub locally. The following commands show how to run the API process. docker build -t txtai.api -f docker/api.Dockerfile . docker run --name txtai.api -p 8000 :8000 --rm -it txtai.api # Alternatively, if nvidia-docker is installed, the build will support GPU runtimes docker run --name txtai.api --runtime = nvidia -p 8000 :8000 --rm -it txtai.api This will bring up an API instance without having to install Python, txtai or any dependencies on your machine! Distributed embeddings clusters The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below. cluster : shards : - http://127.0.0.1:8002 - http://127.0.0.1:8003 This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters. This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index. Differences between Python and API The txtai API provides all the major functionality found in this project. But there are differences due to the nature of JSON and differences across the supported programming languages. For example, any Python callable method is available at a named endpoint (i.e. instead of summary() the method call would be summary.summary()). Return types vary as tuples are returned as objects via the API. Supported language bindings The following programming languages have txtai bindings: JavaScript Java Rust Go See each of the projects above for details on how to install and use. Please add an issue to request additional language bindings! application FastAPI application module get () Returns global API instance. Returns: Type Description API instance Source code in txtai/api/application.py def get (): \"\"\" Returns global API instance. Returns: API instance \"\"\" # pylint: disable=W0603 global INSTANCE return INSTANCE start () FastAPI startup event. Loads API instance. Source code in txtai/api/application.py @app . on_event ( \"startup\" ) def start (): \"\"\" FastAPI startup event. Loads API instance. \"\"\" # pylint: disable=W0603 global INSTANCE # Load YAML settings with open ( os . getenv ( \"CONFIG\" ), \"r\" ) as f : # Read configuration config = yaml . safe_load ( f ) # Instantiate API instance api = os . getenv ( \"API_CLASS\" ) INSTANCE = Factory . create ( config , api ) if api else API ( config ) # Router definitions routers = [ ( \"embeddings\" , embeddings . router ), ( \"extractor\" , extractor . router ), ( \"labels\" , labels . router ), ( \"segmentation\" , segmentation . router ), ( \"similarity\" , similarity . router ), ( \"summary\" , summary . router ), ( \"textractor\" , textractor . router ), ( \"transcription\" , transcription . router ), ( \"translation\" , translation . router ), ( \"workflow\" , workflow . router ), ] # Conditionally add routes based on configuration for name , router in routers : if name in config : app . include_router ( router ) # Special case for embeddings clusters if \"cluster\" in config and \"embeddings\" not in config : app . include_router ( embeddings . router ) # Special case to add similarity instance for embeddings if \"embeddings\" in config and \"similarity\" not in config : app . include_router ( similarity . router ) # Execute extensions if present extensions = os . getenv ( \"EXTENSIONS\" ) if extensions : for extension in extensions . split ( \",\" ): # Create instance and execute extension extension = Factory . get ( extension . strip ())() extension ( app )","title":"API"},{"location":"api/#api","text":"txtai has a full-featured API that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note that this configuration file enables all functionality. It is suggested that separate processes are used for each instance of a txtai component. Components can be joined together with workflows. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Enbeddings index embeddings : path : sentence-transformers/nli-mpnet-base-v2 # Extractive QA extractor : path : distilbert-base-cased-distilled-squad # Zero-shot labeling labels : # Similarity similarity : # Text segmentation segmentation : sentences : true # Text summarization summary : # Text extraction textractor : paragraphs : true minlength : 100 join : true # Transcribe audio to text transcription : # Translate text between languages translation : # Workflow definitions workflow : sumfrench : tasks : - action : textractor task : storage ids : false - action : summary - action : translation args : [ \"fr\" ] sumspanish : tasks : - action : textractor task : url - action : summary - action : translation args : [ \"es\" ] Assuming this YAML content is stored in a file named index.yml, the following command starts the API process. CONFIG=index.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details.","title":"API"},{"location":"api/#docker","text":"A Dockerfile with commands to install txtai, all dependencies and default configuration is available in this repository. The Dockerfile can be copied from the docker directory on GitHub locally. The following commands show how to run the API process. docker build -t txtai.api -f docker/api.Dockerfile . docker run --name txtai.api -p 8000 :8000 --rm -it txtai.api # Alternatively, if nvidia-docker is installed, the build will support GPU runtimes docker run --name txtai.api --runtime = nvidia -p 8000 :8000 --rm -it txtai.api This will bring up an API instance without having to install Python, txtai or any dependencies on your machine!","title":"Docker"},{"location":"api/#distributed-embeddings-clusters","text":"The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below. cluster : shards : - http://127.0.0.1:8002 - http://127.0.0.1:8003 This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters. This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index.","title":"Distributed embeddings clusters"},{"location":"api/#differences-between-python-and-api","text":"The txtai API provides all the major functionality found in this project. But there are differences due to the nature of JSON and differences across the supported programming languages. For example, any Python callable method is available at a named endpoint (i.e. instead of summary() the method call would be summary.summary()). Return types vary as tuples are returned as objects via the API.","title":"Differences between Python and API"},{"location":"api/#supported-language-bindings","text":"The following programming languages have txtai bindings: JavaScript Java Rust Go See each of the projects above for details on how to install and use. Please add an issue to request additional language bindings!","title":"Supported language bindings"},{"location":"api/#txtai.api.application","text":"FastAPI application module","title":"application"},{"location":"api/#txtai.api.application.get","text":"Returns global API instance. Returns: Type Description API instance Source code in txtai/api/application.py def get (): \"\"\" Returns global API instance. Returns: API instance \"\"\" # pylint: disable=W0603 global INSTANCE return INSTANCE","title":"get()"},{"location":"api/#txtai.api.application.start","text":"FastAPI startup event. Loads API instance. Source code in txtai/api/application.py @app . on_event ( \"startup\" ) def start (): \"\"\" FastAPI startup event. Loads API instance. \"\"\" # pylint: disable=W0603 global INSTANCE # Load YAML settings with open ( os . getenv ( \"CONFIG\" ), \"r\" ) as f : # Read configuration config = yaml . safe_load ( f ) # Instantiate API instance api = os . getenv ( \"API_CLASS\" ) INSTANCE = Factory . create ( config , api ) if api else API ( config ) # Router definitions routers = [ ( \"embeddings\" , embeddings . router ), ( \"extractor\" , extractor . router ), ( \"labels\" , labels . router ), ( \"segmentation\" , segmentation . router ), ( \"similarity\" , similarity . router ), ( \"summary\" , summary . router ), ( \"textractor\" , textractor . router ), ( \"transcription\" , transcription . router ), ( \"translation\" , translation . router ), ( \"workflow\" , workflow . router ), ] # Conditionally add routes based on configuration for name , router in routers : if name in config : app . include_router ( router ) # Special case for embeddings clusters if \"cluster\" in config and \"embeddings\" not in config : app . include_router ( embeddings . router ) # Special case to add similarity instance for embeddings if \"embeddings\" in config and \"similarity\" not in config : app . include_router ( similarity . router ) # Execute extensions if present extensions = os . getenv ( \"EXTENSIONS\" ) if extensions : for extension in extensions . split ( \",\" ): # Create instance and execute extension extension = Factory . get ( extension . strip ())() extension ( app )","title":"start()"},{"location":"embeddings/","text":"Embeddings An Embeddings instance is the engine that provides similarity search. Embeddings can be used to run ad-hoc similarity comparisions or build/search large indices. Embeddings parameters are set through the constructor. Examples below. # Transformers embeddings model Embeddings ({ \"method\" : \"transformers\" , \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) # Word embeddings model Embeddings ({ \"method\" : \"words\" , \"path\" : vectors , \"storevectors\" : True , \"scoring\" : \"bm25\" , \"pca\" : 3 , \"quantize\" : True }) Configuration method method : transformers|sentence-transformers|words Sentence embeddings method to use. Options listed below. transformers Builds sentence embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build sentence embeddings. sentence-transformers Same as transformers but loads models with the sentence-transformers library. words Builds sentence embeddings using a word embeddings model. sentence-transformers and words require the similarity extras package to be installed. The method is inferred using the path if not provided. path path : string Required field that sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Model Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model. backend backend : faiss|annoy|hnsw Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss. Additional backends require the similarity extras package to be installed. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted. faiss faiss : components : Comma separated list of components - defaults to None nprobe : search probe setting (int) - defaults to 6 See Faiss documentation on the index factory and search for more information on these parameters. annoy annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters. hnsw hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters. quantize quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization. Additional configuration for Transformers models tokenize tokenize : boolean Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text and may increase the quality of English language sentence embeddings in some situations. Additional configuration for Word embedding models Word embeddings provide a good tradeoff of performance to functionality for a similarity search system. With that being said, Transformers models are making great progress in scaling performance down to smaller models and are the preferred vector backend in txtai for most cases. Word embeddings models require the similarity extras package to be installed. storevectors storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies. scoring scoring : bm25|tfidf|sif A scoring model builds weighted averages of word vectors for a given sentence. Supports BM25, TF-IDF and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built. pca pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied. __init__ ( self , config = None ) special Creates a new Embeddings model. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new Embeddings model. Args: config: embeddings configuration \"\"\" # Configuration self . config = config # Embeddings model self . embeddings = None if self . config and self . config . get ( \"method\" ) != \"transformers\" : # Dimensionality reduction model self . reducer = None # Embedding scoring method - weighs each word in a sentence self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) if self . config and self . config . get ( \"scoring\" ) else None else : self . reducer , self . scoring = None , None # Sentence vectors model self . model = self . loadVectors () if self . config else None batchsearch ( self , queries , limit = 3 ) Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Parameters: Name Type Description Default queries queries text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text|tokens limit: maximum results Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors embeddings = np . array ([ self . transform (( None , query , None )) for query in queries ]) # Search embeddings index results = self . embeddings . search ( embeddings , limit ) # Map ids if id mapping available lookup = self . config . get ( \"ids\" ) if lookup : results = [[( lookup [ i ], score ) for i , score in r ] for r in results ] return results batchsimilarity ( self , queries , texts ) Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Args: queries: queries text|tokens texts: list of text|tokens Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) texts = np . array ([ self . transform (( None , text , None )) for text in texts ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , texts . T ) . tolist () # Add index id and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ] batchtransform ( self , documents ) Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Args: documents: list of (id, text|tokens, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ] index ( self , documents ) Builds an embeddings index. This method overwrites an existing index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def index ( self , documents ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, text|tokens, tags) \"\"\" # Transform documents to embeddings vectors ids , dimensions , embeddings = self . vectors ( documents ) # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save embeddings metadata self . config [ \"ids\" ] = ids self . config [ \"dimensions\" ] = dimensions # Create embeddings index self . embeddings = ANNFactory . create ( self . config ) # Build the index self . embeddings . index ( embeddings ) load ( self , path ) Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Parameters: Name Type Description Default path input directory path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Args: path: input directory path \"\"\" # Index configuration with open ( \" %s /config\" % path , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Sentence embeddings index self . embeddings = ANNFactory . create ( self . config ) self . embeddings . load ( \" %s /embeddings\" % path ) # Dimensionality reduction if self . config . get ( \"pca\" ): with open ( \" %s /lsa\" % path , \"rb\" ) as handle : self . reducer = Reducer () self . reducer . load ( path ) # Embedding scoring if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( path ) # Sentence vectors model - transforms text into sentence embeddings self . model = self . loadVectors () save ( self , path ) Saves a model. Parameters: Name Type Description Default path output directory path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves a model. Args: path: output directory path \"\"\" if self . config : # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy vectors file if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( \" %s /config\" % path , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Write sentence embeddings index self . embeddings . save ( \" %s /embeddings\" % path ) # Save dimensionality reduction if self . reducer : self . reducer . save ( path ) # Save embedding scoring if self . scoring : self . scoring . save ( path ) score ( self , documents ) Builds a scoring index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Args: documents: list of (id, text|tokens, tags) \"\"\" if self . scoring : # Build scoring index over documents self . scoring . index ( documents ) search ( self , query , limit = 3 ) Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Parameters: Name Type Description Default query query text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Args: query: query text|tokens limit: maximum results Returns: list of (id, score) \"\"\" return self . batchsearch ([ query ], limit )[ 0 ] similarity ( self , query , texts ) Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Args: query: query text|tokens texts: list of text|tokens Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], texts )[ 0 ] transform ( self , document ) Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default document (id, text|tokens, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Args: document: (id, text|tokens, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding","title":"Embeddings"},{"location":"embeddings/#embeddings","text":"An Embeddings instance is the engine that provides similarity search. Embeddings can be used to run ad-hoc similarity comparisions or build/search large indices. Embeddings parameters are set through the constructor. Examples below. # Transformers embeddings model Embeddings ({ \"method\" : \"transformers\" , \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) # Word embeddings model Embeddings ({ \"method\" : \"words\" , \"path\" : vectors , \"storevectors\" : True , \"scoring\" : \"bm25\" , \"pca\" : 3 , \"quantize\" : True })","title":"Embeddings"},{"location":"embeddings/#configuration","text":"","title":"Configuration"},{"location":"embeddings/#method","text":"method : transformers|sentence-transformers|words Sentence embeddings method to use. Options listed below.","title":"method"},{"location":"embeddings/#transformers","text":"Builds sentence embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build sentence embeddings.","title":"transformers"},{"location":"embeddings/#sentence-transformers","text":"Same as transformers but loads models with the sentence-transformers library.","title":"sentence-transformers"},{"location":"embeddings/#words","text":"Builds sentence embeddings using a word embeddings model. sentence-transformers and words require the similarity extras package to be installed. The method is inferred using the path if not provided.","title":"words"},{"location":"embeddings/#path","text":"path : string Required field that sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Model Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model.","title":"path"},{"location":"embeddings/#backend","text":"backend : faiss|annoy|hnsw Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss. Additional backends require the similarity extras package to be installed. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted.","title":"backend"},{"location":"embeddings/#faiss","text":"faiss : components : Comma separated list of components - defaults to None nprobe : search probe setting (int) - defaults to 6 See Faiss documentation on the index factory and search for more information on these parameters.","title":"faiss"},{"location":"embeddings/#annoy","text":"annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters.","title":"annoy"},{"location":"embeddings/#hnsw","text":"hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters.","title":"hnsw"},{"location":"embeddings/#quantize","text":"quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization.","title":"quantize"},{"location":"embeddings/#additional-configuration-for-transformers-models","text":"","title":"Additional configuration for Transformers models"},{"location":"embeddings/#tokenize","text":"tokenize : boolean Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text and may increase the quality of English language sentence embeddings in some situations.","title":"tokenize"},{"location":"embeddings/#additional-configuration-for-word-embedding-models","text":"Word embeddings provide a good tradeoff of performance to functionality for a similarity search system. With that being said, Transformers models are making great progress in scaling performance down to smaller models and are the preferred vector backend in txtai for most cases. Word embeddings models require the similarity extras package to be installed.","title":"Additional configuration for Word embedding models"},{"location":"embeddings/#storevectors","text":"storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies.","title":"storevectors"},{"location":"embeddings/#scoring","text":"scoring : bm25|tfidf|sif A scoring model builds weighted averages of word vectors for a given sentence. Supports BM25, TF-IDF and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built.","title":"scoring"},{"location":"embeddings/#pca","text":"pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied.","title":"pca"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.__init__","text":"Creates a new Embeddings model. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new Embeddings model. Args: config: embeddings configuration \"\"\" # Configuration self . config = config # Embeddings model self . embeddings = None if self . config and self . config . get ( \"method\" ) != \"transformers\" : # Dimensionality reduction model self . reducer = None # Embedding scoring method - weighs each word in a sentence self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) if self . config and self . config . get ( \"scoring\" ) else None else : self . reducer , self . scoring = None , None # Sentence vectors model self . model = self . loadVectors () if self . config else None","title":"__init__()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchsearch","text":"Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Parameters: Name Type Description Default queries queries text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of (id, score) sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text|tokens limit: maximum results Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors embeddings = np . array ([ self . transform (( None , query , None )) for query in queries ]) # Search embeddings index results = self . embeddings . search ( embeddings , limit ) # Map ids if id mapping available lookup = self . config . get ( \"ids\" ) if lookup : results = [[( lookup [ i ], score ) for i , score in r ] for r in results ] return results","title":"batchsearch()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchsimilarity","text":"Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Args: queries: queries text|tokens texts: list of text|tokens Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) texts = np . array ([ self . transform (( None , text , None )) for text in texts ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , texts . T ) . tolist () # Add index id and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ]","title":"batchsimilarity()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchtransform","text":"Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Args: documents: list of (id, text|tokens, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ]","title":"batchtransform()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.index","text":"Builds an embeddings index. This method overwrites an existing index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def index ( self , documents ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, text|tokens, tags) \"\"\" # Transform documents to embeddings vectors ids , dimensions , embeddings = self . vectors ( documents ) # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save embeddings metadata self . config [ \"ids\" ] = ids self . config [ \"dimensions\" ] = dimensions # Create embeddings index self . embeddings = ANNFactory . create ( self . config ) # Build the index self . embeddings . index ( embeddings )","title":"index()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.load","text":"Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Parameters: Name Type Description Default path input directory path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads a pre-trained model. Models have the following files: config - configuration embeddings - sentence embeddings index lsa - LSA model, used to remove the principal component(s) scoring - scoring model used to weigh word vectors vectors - vectors model Args: path: input directory path \"\"\" # Index configuration with open ( \" %s /config\" % path , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Sentence embeddings index self . embeddings = ANNFactory . create ( self . config ) self . embeddings . load ( \" %s /embeddings\" % path ) # Dimensionality reduction if self . config . get ( \"pca\" ): with open ( \" %s /lsa\" % path , \"rb\" ) as handle : self . reducer = Reducer () self . reducer . load ( path ) # Embedding scoring if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( path ) # Sentence vectors model - transforms text into sentence embeddings self . model = self . loadVectors ()","title":"load()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.save","text":"Saves a model. Parameters: Name Type Description Default path output directory path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves a model. Args: path: output directory path \"\"\" if self . config : # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy vectors file if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( \" %s /config\" % path , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Write sentence embeddings index self . embeddings . save ( \" %s /embeddings\" % path ) # Save dimensionality reduction if self . reducer : self . reducer . save ( path ) # Save embedding scoring if self . scoring : self . scoring . save ( path )","title":"save()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.score","text":"Builds a scoring index. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Args: documents: list of (id, text|tokens, tags) \"\"\" if self . scoring : # Build scoring index over documents self . scoring . index ( documents )","title":"score()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.search","text":"Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Parameters: Name Type Description Default query query text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of (id, score) sorted by highest score, where id is the document id in the embeddings model. Args: query: query text|tokens limit: maximum results Returns: list of (id, score) \"\"\" return self . batchsearch ([ query ], limit )[ 0 ]","title":"search()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.similarity","text":"Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Args: query: query text|tokens texts: list of text|tokens Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], texts )[ 0 ]","title":"similarity()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.transform","text":"Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default document (id, text|tokens, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Args: document: (id, text|tokens, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding","title":"transform()"},{"location":"examples/","text":"Examples The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below. Semantic Search Build semantic/similarity/vector search applications. Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems API Gallery Using txtai in JavaScript, Java, Rust and Go Similarity search with images Embed images and text into the same space for search Distributed embeddings cluster Distribute an embeddings index across multiple data nodes Pipelines and Workflows NLP-backed data transformation pipelines and workflows. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Run pipeline workflows Simple yet powerful constructs to efficiently process data Model Training Train NLP models. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Applications Series of example applications with txtai. Application Description Demo query shell Basic similarity search example. Used in the original txtai demo. Book search Book similarity search application. Index book descriptions and query using natural language statements. Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows.","title":"Examples"},{"location":"examples/#examples","text":"The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below.","title":"Examples"},{"location":"examples/#semantic-search","text":"Build semantic/similarity/vector search applications. Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems API Gallery Using txtai in JavaScript, Java, Rust and Go Similarity search with images Embed images and text into the same space for search Distributed embeddings cluster Distribute an embeddings index across multiple data nodes","title":"Semantic Search"},{"location":"examples/#pipelines-and-workflows","text":"NLP-backed data transformation pipelines and workflows. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Run pipeline workflows Simple yet powerful constructs to efficiently process data","title":"Pipelines and Workflows"},{"location":"examples/#model-training","text":"Train NLP models. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust","title":"Model Training"},{"location":"examples/#applications","text":"Series of example applications with txtai. Application Description Demo query shell Basic similarity search example. Used in the original txtai demo. Book search Book similarity search application. Index book descriptions and query using natural language statements. Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows.","title":"Applications"},{"location":"install/","text":"Installation The easiest way to install is via pip and PyPI pip install txtai Python 3.6+ is supported. Using a Python virtual environment is recommended. Install from source txtai can also be installed directly from GitHub to access the latest, unreleased features. pip install git+https://github.com/neuml/txtai Environment specific prerequisites Additional environment specific prerequisites are below. Linux Optional audio transcription requires a system library to be installed macOS Run brew install libomp see this link Windows Optional dependencies require C++ Build Tools Optional dependencies txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections. All Install all dependencies (mirrors txtai < 3.2) pip install txtai[all] API Serve txtai via a web API. pip install txtai[api] Pipeline All pipelines - default install comes with most common pipelines. pip install txtai[pipeline] Similarity Word vectors, support for sentence-transformers models not on the HF Hub and additional ANN libraries. pip install txtai[similarity] Workflow All workflow tasks - default install comes with most common workflow tasks. pip install txtai[workflow] Multiple dependencies can be specified at the same time. pip install txtai[pipeline,workflow]","title":"Installation"},{"location":"install/#installation","text":"The easiest way to install is via pip and PyPI pip install txtai Python 3.6+ is supported. Using a Python virtual environment is recommended.","title":"Installation"},{"location":"install/#install-from-source","text":"txtai can also be installed directly from GitHub to access the latest, unreleased features. pip install git+https://github.com/neuml/txtai","title":"Install from source"},{"location":"install/#environment-specific-prerequisites","text":"Additional environment specific prerequisites are below.","title":"Environment specific prerequisites"},{"location":"install/#linux","text":"Optional audio transcription requires a system library to be installed","title":"Linux"},{"location":"install/#macos","text":"Run brew install libomp see this link","title":"macOS"},{"location":"install/#windows","text":"Optional dependencies require C++ Build Tools","title":"Windows"},{"location":"install/#optional-dependencies","text":"txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections.","title":"Optional dependencies"},{"location":"install/#all","text":"Install all dependencies (mirrors txtai < 3.2) pip install txtai[all]","title":"All"},{"location":"install/#api","text":"Serve txtai via a web API. pip install txtai[api]","title":"API"},{"location":"install/#pipeline","text":"All pipelines - default install comes with most common pipelines. pip install txtai[pipeline]","title":"Pipeline"},{"location":"install/#similarity","text":"Word vectors, support for sentence-transformers models not on the HF Hub and additional ANN libraries. pip install txtai[similarity]","title":"Similarity"},{"location":"install/#workflow","text":"All workflow tasks - default install comes with most common workflow tasks. pip install txtai[workflow] Multiple dependencies can be specified at the same time. pip install txtai[pipeline,workflow]","title":"Workflow"},{"location":"why/","text":"Why txtai? In addition to traditional search systems, a growing number of semantic search solutions are available, so why txtai? pip install txtai is all that is needed to get started Works well with both small and big data, prototypes can be built in a couple lines of code, scale up as needed Rich data processing framework (pipelines and workflows) to pre and post process data Work in your programming language of choice via the API Modular with low install footprint, most dependencies optional and only required when you need them Learn by example, notebooks cover all available functionality","title":"Why txtai?"},{"location":"why/#why-txtai","text":"In addition to traditional search systems, a growing number of semantic search solutions are available, so why txtai? pip install txtai is all that is needed to get started Works well with both small and big data, prototypes can be built in a couple lines of code, scale up as needed Rich data processing framework (pipelines and workflows) to pre and post process data Work in your programming language of choice via the API Modular with low install footprint, most dependencies optional and only required when you need them Learn by example, notebooks cover all available functionality","title":"Why txtai?"},{"location":"workflows/","text":"Workflows Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows don't know they are working with pipelines but enable efficient processing of pipeline data. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. An example Workflow is shown below. This workflow will work with both documents and audio files. Documents will have text extracted and summarized. Audio files will be transcribed. Both results will be joined, translated into French and loaded into an Embeddings index. # file:// prefixes are required to signal to the workflow this is a file and not a text string files = [ \"file://txtai/article.pdf\" , \"file://txtai/US_tops_5_million.wav\" , \"file://txtai/Canadas_last_fully.wav\" , \"file://txtai/Beijing_mobilises.wav\" , \"file://txtai/The_National_Park.wav\" , \"file://txtai/Maine_man_wins_1_mil.wav\" , \"file://txtai/Make_huge_profits.wav\" ] data = [( x , element , None ) for x , element in enumerate ( files )] # Workflow that extracts text and builds a summary articles = Workflow ([ FileTask ( textractor ), Task ( lambda x : summary ([ y [: 1024 ] for y in x ])) ]) # Define workflow tasks. Workflows can also be tasks! tasks = [ WorkflowTask ( articles , r \".\\.pdf$\" ), FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )), Task ( index , unpack = False ) ] # Workflow that translates text to French workflow = Workflow ( tasks ) for _ in workflow ( data ): pass __init__ ( self , tasks , batch = 100 ) special Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 \"\"\" self . tasks = tasks self . batch = batch __call__ ( self , elements ) special Executes a workflow for input elements. Parameters: Name Type Description Default elements list of data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: list of data elements Returns: transformed data elements \"\"\" batch = [] for x in elements : batch . append ( x ) if len ( batch ) == self . batch : yield from self . process ( batch ) batch = [] if batch : yield from self . process ( batch ) Tasks __init__ ( self , action = None , select = None , unpack = True ) special Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Parameters: Name Type Description Default action action to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Args: action: action to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples \"\"\" self . action = action self . select = select self . unpack = unpack File Task Task that processes file urls Image Task Task that processes image urls Service Task Task that runs content against a http service Storage Task Task that expands a local directory or cloud storage bucket into a list of URLs to process URL Task Task that processes urls Workflow Task Task that runs a Workflow. Allows creating workflows of workflows.","title":"Workflows"},{"location":"workflows/#workflows","text":"Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows don't know they are working with pipelines but enable efficient processing of pipeline data. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. An example Workflow is shown below. This workflow will work with both documents and audio files. Documents will have text extracted and summarized. Audio files will be transcribed. Both results will be joined, translated into French and loaded into an Embeddings index. # file:// prefixes are required to signal to the workflow this is a file and not a text string files = [ \"file://txtai/article.pdf\" , \"file://txtai/US_tops_5_million.wav\" , \"file://txtai/Canadas_last_fully.wav\" , \"file://txtai/Beijing_mobilises.wav\" , \"file://txtai/The_National_Park.wav\" , \"file://txtai/Maine_man_wins_1_mil.wav\" , \"file://txtai/Make_huge_profits.wav\" ] data = [( x , element , None ) for x , element in enumerate ( files )] # Workflow that extracts text and builds a summary articles = Workflow ([ FileTask ( textractor ), Task ( lambda x : summary ([ y [: 1024 ] for y in x ])) ]) # Define workflow tasks. Workflows can also be tasks! tasks = [ WorkflowTask ( articles , r \".\\.pdf$\" ), FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )), Task ( index , unpack = False ) ] # Workflow that translates text to French workflow = Workflow ( tasks ) for _ in workflow ( data ): pass","title":"Workflows"},{"location":"workflows/#txtai.workflow.base.Workflow.__init__","text":"Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 \"\"\" self . tasks = tasks self . batch = batch","title":"__init__()"},{"location":"workflows/#txtai.workflow.base.Workflow.__call__","text":"Executes a workflow for input elements. Parameters: Name Type Description Default elements list of data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: list of data elements Returns: transformed data elements \"\"\" batch = [] for x in elements : batch . append ( x ) if len ( batch ) == self . batch : yield from self . process ( batch ) batch = [] if batch : yield from self . process ( batch )","title":"__call__()"},{"location":"workflows/#tasks","text":"","title":"Tasks"},{"location":"workflows/#txtai.workflow.task.base.Task.__init__","text":"Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Parameters: Name Type Description Default action action to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Actions are callable functions. Args: action: action to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples \"\"\" self . action = action self . select = select self . unpack = unpack","title":"__init__()"},{"location":"workflows/#file-task","text":"Task that processes file urls","title":"File Task"},{"location":"workflows/#image-task","text":"Task that processes image urls","title":"Image Task"},{"location":"workflows/#service-task","text":"Task that runs content against a http service","title":"Service Task"},{"location":"workflows/#storage-task","text":"Task that expands a local directory or cloud storage bucket into a list of URLs to process","title":"Storage Task"},{"location":"workflows/#url-task","text":"Task that processes urls","title":"URL Task"},{"location":"workflows/#workflow-task","text":"Task that runs a Workflow. Allows creating workflows of workflows.","title":"Workflow Task"},{"location":"pipelines/extractor/","text":"Extractor An Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model. Extractor parameters are set as constructor arguments. Examples below. Extractor ( embeddings , path , quantize , gpu , model , tokenizer ) __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None ) special Builds a new extractor. Parameters: Name Type Description Default similarity similarity instance (embeddings or similarity instance) required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) True model optional existing pipeline model to wrap None tokenizer Tokenizer class None Source code in txtai/pipeline/extractor.py def __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None ): \"\"\" Builds a new extractor. Args: similarity: similarity instance (embeddings or similarity instance) path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class \"\"\" # Similarity instance self . similarity = similarity # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer # Minimum score to include context match self . minscore = minscore if minscore is not None else 0.0 # Minimum number of tokens to include context match self . mintokens = mintokens if mintokens is not None else 0.0 __call__ ( self , queue , texts ) special Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , topns , snippets = [], [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: 3 ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) topns . append ([ text for _ , text , _ in topn ]) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , topns , snippets )","title":"Extractor"},{"location":"pipelines/extractor/#extractor","text":"An Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model. Extractor parameters are set as constructor arguments. Examples below. Extractor ( embeddings , path , quantize , gpu , model , tokenizer )","title":"Extractor"},{"location":"pipelines/extractor/#txtai.pipeline.extractor.Extractor.__init__","text":"Builds a new extractor. Parameters: Name Type Description Default similarity similarity instance (embeddings or similarity instance) required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) True model optional existing pipeline model to wrap None tokenizer Tokenizer class None Source code in txtai/pipeline/extractor.py def __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None ): \"\"\" Builds a new extractor. Args: similarity: similarity instance (embeddings or similarity instance) path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class \"\"\" # Similarity instance self . similarity = similarity # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer # Minimum score to include context match self . minscore = minscore if minscore is not None else 0.0 # Minimum number of tokens to include context match self . mintokens = mintokens if mintokens is not None else 0.0","title":"__init__()"},{"location":"pipelines/extractor/#txtai.pipeline.extractor.Extractor.__call__","text":"Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , topns , snippets = [], [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: 3 ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) topns . append ([ text for _ , text , _ in topn ]) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , topns , snippets )","title":"__call__()"},{"location":"pipelines/labels/","text":"Labels A Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling). Labels parameters are set as constructor arguments. Examples below. # Default configuration Labels () # Custom model with zero shot classification Labels ( \"roberta-large-mnli\" ) # Model with fixed labels Labels ( dynamic = False ) __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ) special Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic __call__ ( self , text , labels = None , multilabel = False , workers = 0 ) special Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels None multilabel labels are independent if True, otherwise scores are normalized to sum to 1 per text item False workers number of parallel workers to use for processing data, defaults to none 0 Returns: Type Description list of (id, score) Source code in txtai/pipeline/labels.py def __call__ ( self , text , labels = None , multilabel = False , workers = 0 ): \"\"\" Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels multilabel: labels are independent if True, otherwise scores are normalized to sum to 1 per text item workers: number of parallel workers to use for processing data, defaults to none Returns: list of (id, score) \"\"\" if self . dynamic : # Run zero shot classification pipeline results = self . pipeline ( text , labels , multi_label = multilabel , truncation = True , num_workers = workers ) else : # Set classification function based on inputs function = \"sigmoid\" if multilabel or len ( self . labels ()) == 1 else \"softmax\" # Run text classification pipeline results = self . pipeline ( text , return_all_scores = True , function_to_apply = function , num_workers = workers ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of (id, score) scores = [] for result in results : if self . dynamic : scores . append ([( labels . index ( label ), result [ \"scores\" ][ x ]) for x , label in enumerate ( result [ \"labels\" ])]) else : # Filter results using labels, if provided result = self . limit ( result , labels ) scores . append ( sorted ( enumerate ( result ), key = lambda x : x [ 1 ], reverse = True )) return scores [ 0 ] if isinstance ( text , str ) else scores","title":"Labels"},{"location":"pipelines/labels/#labels","text":"A Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling). Labels parameters are set as constructor arguments. Examples below. # Default configuration Labels () # Custom model with zero shot classification Labels ( \"roberta-large-mnli\" ) # Model with fixed labels Labels ( dynamic = False )","title":"Labels"},{"location":"pipelines/labels/#txtai.pipeline.labels.Labels.__init__","text":"Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic","title":"__init__()"},{"location":"pipelines/labels/#txtai.pipeline.labels.Labels.__call__","text":"Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels None multilabel labels are independent if True, otherwise scores are normalized to sum to 1 per text item False workers number of parallel workers to use for processing data, defaults to none 0 Returns: Type Description list of (id, score) Source code in txtai/pipeline/labels.py def __call__ ( self , text , labels = None , multilabel = False , workers = 0 ): \"\"\" Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels multilabel: labels are independent if True, otherwise scores are normalized to sum to 1 per text item workers: number of parallel workers to use for processing data, defaults to none Returns: list of (id, score) \"\"\" if self . dynamic : # Run zero shot classification pipeline results = self . pipeline ( text , labels , multi_label = multilabel , truncation = True , num_workers = workers ) else : # Set classification function based on inputs function = \"sigmoid\" if multilabel or len ( self . labels ()) == 1 else \"softmax\" # Run text classification pipeline results = self . pipeline ( text , return_all_scores = True , function_to_apply = function , num_workers = workers ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of (id, score) scores = [] for result in results : if self . dynamic : scores . append ([( labels . index ( label ), result [ \"scores\" ][ x ]) for x , label in enumerate ( result [ \"labels\" ])]) else : # Filter results using labels, if provided result = self . limit ( result , labels ) scores . append ( sorted ( enumerate ( result ), key = lambda x : x [ 1 ], reverse = True )) return scores [ 0 ] if isinstance ( text , str ) else scores","title":"__call__()"},{"location":"pipelines/onnx/","text":"HFOnnx Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation). Example on how to use the pipeline below. from txtai.pipeline import HFOnnx , Labels # Model path path = \"distilbert-base-uncased-finetuned-sst-2-english\" # Export model to ONNX onnx = HFOnnx () model = onnx ( path , \"text-classification\" , \"model.onnx\" , True ) # Run inference and validate labels = Labels (( model , path ), dynamic = False ) labels ( \"I am happy\" ) __init__ ( / , self , * args , ** kwargs ) special __call__ ( self , path , task = 'default' , output = None , quantize = False , opset = 12 ) special Exports a Hugging Face Transformer model to ONNX. Parameters: Name Type Description Default path path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required task optional model task or category, determines the model type and outputs, defaults to export hidden state 'default' output optional output model path, defaults to return byte array if None None quantize if model should be quantized (requires onnx to be installed), defaults to False False opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/hfonnx.py def __call__ ( self , path , task = \"default\" , output = None , quantize = False , opset = 12 ): \"\"\" Exports a Hugging Face Transformer model to ONNX. Args: path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple task: optional model task or category, determines the model type and outputs, defaults to export hidden state output: optional output model path, defaults to return byte array if None quantize: if model should be quantized (requires onnx to be installed), defaults to False opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" inputs , outputs , model = self . parameters ( task ) if isinstance ( path , ( list , tuple )): model , tokenizer = path model = model . cpu () else : model = model ( path ) tokenizer = AutoTokenizer . from_pretrained ( path ) # Generate dummy inputs dummy = dict ( tokenizer ([ \"test inputs\" ], return_tensors = \"pt\" )) # Default to BytesIO if no output file provided output = output if output else BytesIO () # Export model to ONNX export ( model , ( dummy ,), output , opset_version = opset , do_constant_folding = True , input_names = list ( inputs . keys ()), output_names = list ( outputs . keys ()), dynamic_axes = dict ( chain ( inputs . items (), outputs . items ())), ) # Quantize model if quantize : if not ONNX_RUNTIME : raise ImportError ( 'onnxruntime is not available - install \"pipeline\" extra to enable' ) output = self . quantization ( output ) if isinstance ( output , BytesIO ): # Reset stream and return bytes output . seek ( 0 ) output = output . read () return output","title":"ONNX"},{"location":"pipelines/onnx/#hfonnx","text":"Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation). Example on how to use the pipeline below. from txtai.pipeline import HFOnnx , Labels # Model path path = \"distilbert-base-uncased-finetuned-sst-2-english\" # Export model to ONNX onnx = HFOnnx () model = onnx ( path , \"text-classification\" , \"model.onnx\" , True ) # Run inference and validate labels = Labels (( model , path ), dynamic = False ) labels ( \"I am happy\" )","title":"HFOnnx"},{"location":"pipelines/onnx/#txtai.pipeline.hfonnx.HFOnnx.__init__","text":"","title":"__init__()"},{"location":"pipelines/onnx/#txtai.pipeline.hfonnx.HFOnnx.__call__","text":"Exports a Hugging Face Transformer model to ONNX. Parameters: Name Type Description Default path path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required task optional model task or category, determines the model type and outputs, defaults to export hidden state 'default' output optional output model path, defaults to return byte array if None None quantize if model should be quantized (requires onnx to be installed), defaults to False False opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/hfonnx.py def __call__ ( self , path , task = \"default\" , output = None , quantize = False , opset = 12 ): \"\"\" Exports a Hugging Face Transformer model to ONNX. Args: path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple task: optional model task or category, determines the model type and outputs, defaults to export hidden state output: optional output model path, defaults to return byte array if None quantize: if model should be quantized (requires onnx to be installed), defaults to False opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" inputs , outputs , model = self . parameters ( task ) if isinstance ( path , ( list , tuple )): model , tokenizer = path model = model . cpu () else : model = model ( path ) tokenizer = AutoTokenizer . from_pretrained ( path ) # Generate dummy inputs dummy = dict ( tokenizer ([ \"test inputs\" ], return_tensors = \"pt\" )) # Default to BytesIO if no output file provided output = output if output else BytesIO () # Export model to ONNX export ( model , ( dummy ,), output , opset_version = opset , do_constant_folding = True , input_names = list ( inputs . keys ()), output_names = list ( outputs . keys ()), dynamic_axes = dict ( chain ( inputs . items (), outputs . items ())), ) # Quantize model if quantize : if not ONNX_RUNTIME : raise ImportError ( 'onnxruntime is not available - install \"pipeline\" extra to enable' ) output = self . quantization ( output ) if isinstance ( output , BytesIO ): # Reset stream and return bytes output . seek ( 0 ) output = output . read () return output","title":"__call__()"},{"location":"pipelines/overview/","text":"Pipelines txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and can process various types of data. Pipelines can wrap machine learning models as well as other processes. The following pipeline types are currently supported. Hugging Face Transformers pipelines Extractive QA Labeling Similarity Summary Hugging Face Transformers models ONNX Trainer Transcription Translation Data processing calls Text extraction","title":"Overview"},{"location":"pipelines/overview/#pipelines","text":"txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and can process various types of data. Pipelines can wrap machine learning models as well as other processes. The following pipeline types are currently supported. Hugging Face Transformers pipelines Extractive QA Labeling Similarity Summary Hugging Face Transformers models ONNX Trainer Transcription Translation Data processing calls Text extraction","title":"Pipelines"},{"location":"pipelines/segmentation/","text":"Segmentation A Segmentation pipeline segments text into semantic units. Segmentation parameters are set as constructor arguments. Examples below. Segmentation () __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ) special Creates a new Segmentation pipeline. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False lines tokenizes text into lines if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/segmentation.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Segmentation pipeline. Args: sentences: tokenize text into sentences if True, defaults to False lines: tokenizes text into lines if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" if not NLTK : raise ImportError ( 'Segmentation pipeline is not available - install \"pipeline\" extra to enable' ) self . sentences = sentences self . lines = lines self . paragraphs = paragraphs self . minlength = minlength self . join = join __call__ ( self , text ) special Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default text text|list required Returns: Type Description segmented text Source code in txtai/pipeline/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"Segmentation"},{"location":"pipelines/segmentation/#segmentation","text":"A Segmentation pipeline segments text into semantic units. Segmentation parameters are set as constructor arguments. Examples below. Segmentation ()","title":"Segmentation"},{"location":"pipelines/segmentation/#txtai.pipeline.segmentation.Segmentation.__init__","text":"Creates a new Segmentation pipeline. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False lines tokenizes text into lines if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/segmentation.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Segmentation pipeline. Args: sentences: tokenize text into sentences if True, defaults to False lines: tokenizes text into lines if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" if not NLTK : raise ImportError ( 'Segmentation pipeline is not available - install \"pipeline\" extra to enable' ) self . sentences = sentences self . lines = lines self . paragraphs = paragraphs self . minlength = minlength self . join = join","title":"__init__()"},{"location":"pipelines/segmentation/#txtai.pipeline.segmentation.Segmentation.__call__","text":"Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default text text|list required Returns: Type Description segmented text Source code in txtai/pipeline/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipelines/similarity/","text":"Similarity A Similarity pipeline is also a zero shot classifier model where the labels are the queries. The results are transposed to get scores per query/label vs scores per input text. Similarity parameters are set as constructor arguments. Examples below. Similarity () Similarity ( \"roberta-large-mnli\" ) __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ) special Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic __call__ ( self , query , texts , multilabel = True ) special Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"Similarity"},{"location":"pipelines/similarity/#similarity","text":"A Similarity pipeline is also a zero shot classifier model where the labels are the queries. The results are transposed to get scores per query/label vs scores per input text. Similarity parameters are set as constructor arguments. Examples below. Similarity () Similarity ( \"roberta-large-mnli\" )","title":"Similarity"},{"location":"pipelines/similarity/#txtai.pipeline.labels.Similarity.__init__","text":"Source code in txtai/pipeline/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic","title":"__init__()"},{"location":"pipelines/similarity/#txtai.pipeline.similarity.Similarity.__call__","text":"Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"__call__()"},{"location":"pipelines/summary/","text":"Summary A Summary pipeline summarizes text. Summary parameters are set as constructor arguments. Examples below. Summary () Summary ( \"sshleifer/distilbart-cnn-12-3\" ) __init__ ( self , path = None , quantize = False , gpu = True , model = None ) special Source code in txtai/pipeline/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model ) __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ) special Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None workers number of parallel workers to use for processing data, defaults to none 0 Returns: Type Description summary text Source code in txtai/pipeline/summary.py def __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary workers: number of parallel workers to use for processing data, defaults to none Returns: summary text \"\"\" # Validate text length greater than max length check = maxlength if maxlength else self . pipeline . model . config . max_length # Skip text shorter than max length texts = text if isinstance ( text , list ) else [ text ] params = [( x , text if len ( text ) >= check else None ) for x , text in enumerate ( texts )] kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength inputs = [ text for _ , text in params if text ] if inputs : # Run summarization pipeline results = self . pipeline ( inputs , num_workers = workers , ** kwargs ) # Pull out summary text results = iter ([ self . clean ( x [ \"summary_text\" ]) for x in results ]) results = [ next ( results ) if text else texts [ x ] for x , text in params ] else : # Return original results = texts return results [ 0 ] if isinstance ( text , str ) else results","title":"Summary"},{"location":"pipelines/summary/#summary","text":"A Summary pipeline summarizes text. Summary parameters are set as constructor arguments. Examples below. Summary () Summary ( \"sshleifer/distilbart-cnn-12-3\" )","title":"Summary"},{"location":"pipelines/summary/#txtai.pipeline.summary.Summary.__init__","text":"Source code in txtai/pipeline/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model )","title":"__init__()"},{"location":"pipelines/summary/#txtai.pipeline.summary.Summary.__call__","text":"Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None workers number of parallel workers to use for processing data, defaults to none 0 Returns: Type Description summary text Source code in txtai/pipeline/summary.py def __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary workers: number of parallel workers to use for processing data, defaults to none Returns: summary text \"\"\" # Validate text length greater than max length check = maxlength if maxlength else self . pipeline . model . config . max_length # Skip text shorter than max length texts = text if isinstance ( text , list ) else [ text ] params = [( x , text if len ( text ) >= check else None ) for x , text in enumerate ( texts )] kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength inputs = [ text for _ , text in params if text ] if inputs : # Run summarization pipeline results = self . pipeline ( inputs , num_workers = workers , ** kwargs ) # Pull out summary text results = iter ([ self . clean ( x [ \"summary_text\" ]) for x in results ]) results = [ next ( results ) if text else texts [ x ] for x , text in params ] else : # Return original results = texts return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipelines/textractor/","text":"Textractor A Textractor pipeline extracts and splits text from documents. Textractor parameters are set as constructor arguments. Examples below. Textractor () __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ) special Source code in txtai/pipeline/textractor.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): if not TIKA : raise ImportError ( 'Textractor pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( sentences , lines , paragraphs , minlength , join ) __call__ ( self , text ) special Source code in txtai/pipeline/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"Textractor"},{"location":"pipelines/textractor/#textractor","text":"A Textractor pipeline extracts and splits text from documents. Textractor parameters are set as constructor arguments. Examples below. Textractor ()","title":"Textractor"},{"location":"pipelines/textractor/#txtai.pipeline.textractor.Textractor.__init__","text":"Source code in txtai/pipeline/textractor.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): if not TIKA : raise ImportError ( 'Textractor pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( sentences , lines , paragraphs , minlength , join )","title":"__init__()"},{"location":"pipelines/textractor/#txtai.pipeline.segmentation.Textractor.__call__","text":"Source code in txtai/pipeline/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipelines/trainer/","text":"HFTrainer Trains a new Hugging Face Transformer model using the Trainer framework. Examples on how to use the trainer below. import pandas as pd from datasets import load_dataset from txtai.pipeline import HFTrainer trainer = HFTrainer () # Pandas DataFrame df = pd . read_csv ( \"training.csv\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , df ) # Hugging Face dataset ds = load_dataset ( \"glue\" , \"sst2\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , ds [ \"train\" ], columns = ( \"sentence\" , \"label\" )) # List of dicts dt = [{ \"text\" : \"sentence 1\" , \"label\" : 0 }, { \"text\" : \"sentence 2\" , \"label\" : 1 }]] model , tokenizer = trainer ( \"bert-base-uncased\" , dt ) # Support additional TrainingArguments model , tokenizer = trainer ( \"bert-base-uncased\" , dt , learning_rate = 3e-5 , num_train_epochs = 5 ) All TrainingArguments are supported as function arguments to the trainer call. __init__ ( / , self , * args , ** kwargs ) special __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = 'text-classification' , ** args ) special Builds a new model using arguments. Parameters: Name Type Description Default base path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required train training data required validation validation data None columns tuple of columns to use for text/label, defaults to (text, None, label) None maxlength maximum sequence length, defaults to tokenizer.model_max_length None stride chunk size for splitting data for QA tasks 128 task optional model task or category, determines the model type, defaults to \"text-classification\" 'text-classification' args training arguments {} Returns: Type Description (model, tokenizer) Source code in txtai/pipeline/hftrainer.py def __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = \"text-classification\" , ** args ): \"\"\" Builds a new model using arguments. Args: base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple train: training data validation: validation data columns: tuple of columns to use for text/label, defaults to (text, None, label) maxlength: maximum sequence length, defaults to tokenizer.model_max_length stride: chunk size for splitting data for QA tasks task: optional model task or category, determines the model type, defaults to \"text-classification\" args: training arguments Returns: (model, tokenizer) \"\"\" # Parse TrainingArguments args = self . parse ( args ) # Set seed for model reproducibility set_seed ( args . seed ) # Load model configuration, tokenizer and max sequence length config , tokenizer , maxlength = self . load ( base , maxlength ) # List of labels (only for classification models) labels = None # Prepare datasets if task == \"question-answering\" : process = Questions ( tokenizer , columns , maxlength , stride ) else : process = Labels ( tokenizer , columns , maxlength ) labels = process . labels ( train ) # Tokenize training and validation data train , validation = process ( train , validation ) # Create model to train model = self . model ( task , base , config , labels ) # Build trainer trainer = Trainer ( model = model , tokenizer = tokenizer , args = args , train_dataset = train , eval_dataset = validation if validation else None ) # Run training trainer . train () # Run evaluation if validation : trainer . evaluate () # Save model outputs if args . should_save : trainer . save_model () trainer . save_state () # Put model in eval mode to disable weight updates and return (model, tokenizer) return ( model . eval (), tokenizer )","title":"Trainer"},{"location":"pipelines/trainer/#hftrainer","text":"Trains a new Hugging Face Transformer model using the Trainer framework. Examples on how to use the trainer below. import pandas as pd from datasets import load_dataset from txtai.pipeline import HFTrainer trainer = HFTrainer () # Pandas DataFrame df = pd . read_csv ( \"training.csv\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , df ) # Hugging Face dataset ds = load_dataset ( \"glue\" , \"sst2\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , ds [ \"train\" ], columns = ( \"sentence\" , \"label\" )) # List of dicts dt = [{ \"text\" : \"sentence 1\" , \"label\" : 0 }, { \"text\" : \"sentence 2\" , \"label\" : 1 }]] model , tokenizer = trainer ( \"bert-base-uncased\" , dt ) # Support additional TrainingArguments model , tokenizer = trainer ( \"bert-base-uncased\" , dt , learning_rate = 3e-5 , num_train_epochs = 5 ) All TrainingArguments are supported as function arguments to the trainer call.","title":"HFTrainer"},{"location":"pipelines/trainer/#txtai.pipeline.hftrainer.HFTrainer.__init__","text":"","title":"__init__()"},{"location":"pipelines/trainer/#txtai.pipeline.hftrainer.HFTrainer.__call__","text":"Builds a new model using arguments. Parameters: Name Type Description Default base path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required train training data required validation validation data None columns tuple of columns to use for text/label, defaults to (text, None, label) None maxlength maximum sequence length, defaults to tokenizer.model_max_length None stride chunk size for splitting data for QA tasks 128 task optional model task or category, determines the model type, defaults to \"text-classification\" 'text-classification' args training arguments {} Returns: Type Description (model, tokenizer) Source code in txtai/pipeline/hftrainer.py def __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = \"text-classification\" , ** args ): \"\"\" Builds a new model using arguments. Args: base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple train: training data validation: validation data columns: tuple of columns to use for text/label, defaults to (text, None, label) maxlength: maximum sequence length, defaults to tokenizer.model_max_length stride: chunk size for splitting data for QA tasks task: optional model task or category, determines the model type, defaults to \"text-classification\" args: training arguments Returns: (model, tokenizer) \"\"\" # Parse TrainingArguments args = self . parse ( args ) # Set seed for model reproducibility set_seed ( args . seed ) # Load model configuration, tokenizer and max sequence length config , tokenizer , maxlength = self . load ( base , maxlength ) # List of labels (only for classification models) labels = None # Prepare datasets if task == \"question-answering\" : process = Questions ( tokenizer , columns , maxlength , stride ) else : process = Labels ( tokenizer , columns , maxlength ) labels = process . labels ( train ) # Tokenize training and validation data train , validation = process ( train , validation ) # Create model to train model = self . model ( task , base , config , labels ) # Build trainer trainer = Trainer ( model = model , tokenizer = tokenizer , args = args , train_dataset = train , eval_dataset = validation if validation else None ) # Run training trainer . train () # Run evaluation if validation : trainer . evaluate () # Save model outputs if args . should_save : trainer . save_model () trainer . save_state () # Put model in eval mode to disable weight updates and return (model, tokenizer) return ( model . eval (), tokenizer )","title":"__call__()"},{"location":"pipelines/transcription/","text":"Transcription A Transcription pipeline converts speech in audio files to text. Transcription parameters are set as constructor arguments. Examples below. Transcription () Transcription ( \"facebook/wav2vec2-large-960h\" ) __init__ ( self , path = 'facebook/wav2vec2-base-960h' , quantize = False , gpu = True , batch = 64 ) special Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device ) __call__ ( self , files ) special Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"Transcription"},{"location":"pipelines/transcription/#transcription","text":"A Transcription pipeline converts speech in audio files to text. Transcription parameters are set as constructor arguments. Examples below. Transcription () Transcription ( \"facebook/wav2vec2-large-960h\" )","title":"Transcription"},{"location":"pipelines/transcription/#txtai.pipeline.transcription.Transcription.__init__","text":"Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device )","title":"__init__()"},{"location":"pipelines/transcription/#txtai.pipeline.transcription.Transcription.__call__","text":"Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"__call__()"},{"location":"pipelines/translation/","text":"Translation A Translation pipeline translates text between languages Translation parameters are set as constructor arguments. Examples below. Translation () __init__ ( self , path = 'facebook/m2m100_418M' , quantize = False , gpu = True , batch = 64 , langdetect = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' ) special Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available () __call__ ( self , texts , target = 'en' , source = None ) special Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"Translation"},{"location":"pipelines/translation/#translation","text":"A Translation pipeline translates text between languages Translation parameters are set as constructor arguments. Examples below. Translation ()","title":"Translation"},{"location":"pipelines/translation/#txtai.pipeline.translation.Translation.__init__","text":"Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available ()","title":"__init__()"},{"location":"pipelines/translation/#txtai.pipeline.translation.Translation.__call__","text":"Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"__call__()"}]}